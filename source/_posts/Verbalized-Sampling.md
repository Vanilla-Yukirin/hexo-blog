---
title: 仅通过一句提示词，就可以让大模型变得更有创造力
tags:
  - LLM
mathjax: true
date: 2025-12-17 14:14:56
categories:
 - Research
description:
photo:
---

现在很多大模型越训练越丧失随机性或者说创造性。这篇论文提出了一个简单且无需训练的方法，只需要加一句提示词，就解决了LLM在经过对齐训练（比如RLHF）后出现了”模式坍塌“（Mode Collapse）问题，即模型的输出变得单一、缺乏创造力。

<!--more-->

## 问题分析：为什么LLM的输出会变得单调

经过了对齐训练后的模型往往会丧失多样性，导致对于同一个问题的回答千篇一律（本质相同）。这在**创意性写作**、**社会模拟**、***多元对齐***、**合成数据**生成等的任务上，会带来非常负面的影响。

以前的人们觉得这是算法导致的，但是这篇论文发现**数据**才是主因。这里的数据指的是，人类标注者——作为一名人类——天生就喜欢”典型“、”常见“的回答（认知心理学中的***典型性偏差***）。这导致模型在训练中只会输出那个概率最高的、最“典型”的回答，丢弃掉了***长尾***的、更有创意的选项。

> 多元对齐（pluralistic alignment）：在设计和训练LLM时不追求与某单一价值观、文化规范或用户偏好对齐，而是讲究包容多种甚至可能会互相冲突的价值体系、文化背景、道德立场或用户需求。
>
> 典型性偏差（typicality bias）：人们在判断某事物是否属于某个类别时，倾向于依据该事物与该类别”典型成员“的相似程度，而不是看客观的统计学概率或者定义好的规则。在这里是说，人类在标注的时候，自然而然的会喜欢自己看过的、熟悉的、觉得比较正常而经典的故事（比如让LLM讲个故事，总是会有”松鼠森林精灵”之类的吧）
>
> 长尾（long tail）：统计学/机器学习的术语，来源于概率分布的形状。在现实世界的分布中，少数的项目出现在频率极高的头部，而大量的项目各自出现的频率很低，但是加起来占比很大，这部分低频项目构成了一个很长的“尾巴”，即长尾。

## 解决方案：Verbalized Sampling（VS）

作者提出了一个超级简答的方案！不需要训练，因为这种方案就是一种提示词策略就好了！

核心思想：**不要让LLM生成一个回答，而是一口气生成一组回答，并且“口头”输出他们的概率。**

原理是这样的：当要求模型输出一个“分布”的时候，LLM会调用预训练阶段的知识，从而恢复被压抑的多样性；而如果只是要求得到仅一个结果时，LLM会坍塌到那个最典型的结果上。

如果不知道怎么做，直接看下面的提示词模板就好啦。

## 提示词模板

作者给出了一个提示词模板：

```
System prompt: You are a helpful assistant. For each query, please generate a set of five possible responses, each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. Please sample at random from the [full distribution / tails of the distribution, such that the probability of each response is less than 0.10].
User prompt: Write a short story about a bear.
```

翻译一下：

```
System prompt: 你是一个乐于助人的助手。对于每一个提问，请生成一组共五个可能的回答，每个回答需包含在一个单独的 <response> 标签内。每个回答都应包含一个 <text> 和一个数值 <probability>。请从 [完整分布 / 分布的尾部，即每个回答的概率需小于 0.10] 中进行随机采样。
User prompt: 写一个关于熊的短故事。
```

## 实验效果：各个领域都很好

作者在多个领域进行了测试，效果显著：

- 创意写作（诗歌、故事、笑话）：VS方法生成的内容多样性比直接提问提高了1.6~2.1倍率，且质量没有下降，甚至在人类的评估中得分更高。
- 对话模拟：在捐款谈判时，VS能模拟出更像真人的行为（包括拒绝，改变主意），而普通的prompt只会生成前篇一律的礼貌回复。
- 开放式回答：在回答“列举美国的一个州”这种问题的时候，VS能覆盖更广泛的真实分布，而不是只盯着加州和德州。
- 合成数据生成：用VS生成的数据去训练小模型，效果比用普通的prompt生成的数据要更好，因为数据更多样化。

### 数据集和测试方法

这里涉及到了若干数据集的测试方法。

#### 创意写作

数据集：

- 诗歌续写：[www.poemhunter.com](https://www.poemhunter.com/)

- 故事创作：BookMIA dataset

- 笑话写作：从Reddit的r/DadJokes数据集中精选了100个主题提示词

对于每个任务，随机算100个数据点，使用VS方法每次生成$k=5$个候选项，总共采样$N=30$个回答。

评估指标：

- 多样性
    - 语义多样性：计算回答之间的Embedding余弦相似度，相似度越低，多样性越高。
    - 词汇多样性：使用ROUGE-L指标。

- 质量
    - 使用Claude-3.7-Sonnet作为裁判模型，根据特定的评分标准（如幽默感、情节丰富度）进行打分。
    - 同时还进行了人类评估。

#### 对话模拟

 模拟一个“被劝说者”，与一个GPT-4.1扮演的“劝说者”进行多轮对话，最后决定是否捐款 。模型基于给定的人设生成回复。对比直接Prompt和VS方法生成的对话结果。

数据集：

- Persuasion ForGood数据集，包含1017组关于劝说向“救助儿童会”捐款的对话。筛选后使用了939个有效样本。

测试流程：

- LLM扮演“被劝说者”，以相同的人设（比如“手头紧但有爱心”）开始扮演。
- 让GPT-4.1扮演“劝说者”，负责发起对话并劝说捐款。
- 在每一轮对话中，当轮到LLM（被劝说者）回复时，不直接生成一句话，而是按照VS进行以下操作：
    1. 生成候选项及附带概率
    2. 从这些候选项中挑一个发回给“劝说者”
    3. 挑选的策略可以是按概率加权采样，也尝试了随机均匀采样。

- 对话一直进行到结束，最后记录模型最终决定捐款的金额。

评估指标：

- 捐款金额分布：使用KS检验和L1距离，对比LLM的捐款金额分布与真实人类数据（Persuasion ForGood）的分布差异。

- 语言风格：评估回复的词汇丰富度（Distinct-N）和可读性 。

#### 开放式问答

测试模型在面对有多个正确答案的问题时，能否覆盖更广泛的答案空间。枚举型问答，例如“列举一个美国的州” 。

数据集：

- 改编自CoverageQA基准测试。使用40个问题，每个问题至少20个标准答案。

每个问题采样$N=100$个回答。

评估指标：

- KL散度：对比模型输出的分布与预训练预料分布（使用RedPajama数据集作为代理）的差异。KL散度低说明模型越好地还原了预训练时的知识分布。
- 覆盖率：生成的答案覆盖了多少个不同的正确答案。
- 准确率：确保生成的答案就算多样但是依然是正确的。

#### 合成数据生成

测试VS生成的数据是否比普通数据更能提升小模型的性能。任务：生成数学竞赛题。

数据集：

- 正向样本生成
    - MATH500
    - OlympiadBench
    - Minerva Math
    - 用[huggingface/Math-Verify](https://github.com/huggingface/Math-Verify)在上述三个数据集上进行验证

- 负向样本生成
    - GSM8K

测试方法：

- 正向样本生成
    1. 让LLM（GPT-4.1或Gemini）用VS方法生成1000道新的数学题。
    2. 用这些题去微调较小的模型（Qwen2.5-7B，Qwen3-1.7B，Qwen3-4B）。
    3. 在上述三个数学基准测试集上测试微调后的小模型的解题准确率。准确率越高，说明微调的效果越好，说明微调数据多样性越好。

- 负向样本生成
    1. 从GSM8K中随机选50道题，对每道题使用GPT-4.1生成$k=5$个回答，共采样$N=10$个回答。
    2. 生成的要求是“看似合理但实质错误”的解题过程。
    3. 使用和前面一样的Embedding余弦相似度作为多样性评分方法。

#### 其他辅助测试

- 常识推理：使用SimpleQA数据集，验证VS在增加多样性的同时没有产生幻觉。
- 随机数生成：让模型掷骰子，看分布是否接近均匀分布。
- 安全性：使用StrongReject基准测试，确保VS不会因为探索“长尾分布”而绕开安全护栏。

> StrongReject：一个基准数据集，用来评估模型安全性。包含353条有害提示词。用官方设定的安全裁判机制，由GPT-4.1担任裁判。评估模型是否拒绝有害指令、LLM回答的置信度是否高、LLM回答的是否具体。
>
> 安全护栏：模型经过了对齐训练后形成的一种拒绝执行有害指令的内在机制。

## 其他发现

1. 越强的模型，用上了VS之后，收益会更大。因为它们能更好地理解“概率”并遵循指令。
2. 增加多样性没有让模型变得不安全或者产生幻觉。事实准确性和安全性没有下降。

## 动手测试

我写了个 demo 网站：[Verbalized Sampling Demo](https://yukirin.me/demo/verbalized-sampling)

使用 `deepseek-V3.2(no thinking)` 和 `Qwen3-30B-A3B-Instruct-2507` 作为 llm 模型，使用 `Qwen3-Embedding-0.6B` 作为向量计算模型。计算 `标准方法类内平均距离` 和 `VS方法类内平均距离`，以及提升幅度。

首先，给予提示词 `构造一个问题`，在标准方法和VS方法的系统提示词下，各自生成 5 个问题。这样加起来共计 11 个问题。对于这 11 个问题，使用上述两个模型，在两种提示词下，设置每次生成 5 个 response，并计算这 5 个 resonese 的类内距离。

| **模型**                   | **dpsk**        | **dpsk**        | **qwen3**       | **qwen3**       |
| -------------------------- | --------------- | --------------- | --------------- | --------------- |
| **prompt↓**                | **标准回答**    | **尾部采样**    | **标准回答**    | **尾部采样**    |
| 构造一个问题               | 0.7133          | 0.65            | 0.6651          | 0.7147          |
| 什么是量子计算？           | 0.2526          | 0.1914          | 0.2984          | 0.3137          |
| 如何学习编程？             | 0.3334          | 0.4792          | 0.5445          | 0.4623          |
| AI在未来会怎样影响就业？   | 0.3253          | 0.315           | 0.4897          | 0.439           |
| 请解释一下气候变化的原因。 | 0.1793          | 0.4877          | 0.4028          | 0.4877          |
| 推荐几本好书。             | 0.294           | 0.5732          | 0.4553          | 0.4779          |
| 什么是科学？               | 0.261           | 0.2592          | 0.2155          | 0.286           |
| 如何做晚餐？               | 0.4464          | 0.3398          | 0.4029          | 0.3844          |
| 解释气候变化。             | 0.262           | 0.4832          | 0.2811          | 0.3251          |
| 谁发明了电话？             | 0.2416          | 0.1486          | 0.2596          | 0.3564          |
| 如何学会编程？             | 0.2952          | 0.4219          | 0.443           | 0.47            |
| **平均**                   | **0.327645455** | **0.395381818** | **0.405263636** | **0.428836364** |

可以看出，两个模型使用尾部采样后，相比标准回答，均有提升，并且 `deepseek-V3.2(no thinking)` 的提升（20.67%）要大于  `Qwen3-30B-A3B-Instruct-2507`  的提升（0.582%），符合文章结论“越强的模型收益会更大”。

对于构造数学题目数据集的任务，给予提示词`出一道小学奥数题`，设置每次生成 5 个 response，两个模型分别在两种系统提示词下重复生成 10 次。

| 模型     | **dpsk**     | **dpsk**     | **qwen3**    | **qwen3**    |
| -------- | ------------ | ------------ | ------------ | ------------ |
| **序号** | **标准回答** | **尾部采样** | **标准回答** | **尾部采样** |
| 1        | 0.566        | 0.6318       | 0.6481       | 0.6481       |
| 2        | 0.6204       | 0.6293       | 0.5918       | 0.6132       |
| 3        | 0.6047       | 0.5711       | 0.6094       | 0.6739       |
| 4        | 0.5645       | 0.5397       | 0.6192       | 0.5456       |
| 5        | 0.5456       | 0.6249       | 0.527        | 0.652        |
| 6        | 0.588        | 0.6418       | 0.5945       | 0.5622       |
| 7        | 0.535        | 0.6236       | 0.5412       | 0.6411       |
| 8        | 0.5938       | 0.5615       | 0.6586       | 0.6199       |
| 9        | 0.6457       | 0.4604       | 0.5664       | 0.6119       |
| 10       | 0.5822       | 0.6259       | 0.6277       | 0.5005       |
| **平均** | **0.58459**  | **0.591**    | **0.59839**  | **0.60684**  |

**DeepSeek-V3.2**：类内平均距离从 **0.58459** 提升至 **0.5910**，提升幅度约为 **1.10%**。

**Qwen3-30B-A3B**：类内平均距离从 **0.59839** 提升至 **0.60684**，提升幅度约为 **1.41%**。

在逻辑约束较强的数学场景下，VS方法（尾部采样）依然能稳定提升生成样本的多样性，但是效果较小。

> 上述数据均无法通过 t 检验和 U 检验，有待进一步研究……

## References

[[2510.01171] Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity](https://arxiv.org/abs/2510.01171)

[一句 Prompt，让大模型不再千篇一律：Verbalized Sampling 全解析 哔哩哔哩](https://www.bilibili.com/video/BV1yhUGB8E6T/)

